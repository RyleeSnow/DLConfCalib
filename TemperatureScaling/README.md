## 温度缩放 Temperature Scaling

温度缩放是一种简单而有效的后处理校准方法，主要用于校准分类模型的输出概率。它通过引入一个标量参数（称为“温度”）来调整模型的logits（未归一化的预测分数），从而改变softmax函数的输出。

&nbsp;

### 1. 温度缩放的原理
温度缩放的基本思想是通过调整 softmax 函数的“温度”参数来平滑或锐化模型的输出概率。

- 对于分类模型 (classification model) 来说，针对每一个类别，深度学习模型都会输出一个 logit score （假设类别 $i$ 对应的 logit score 为$z_i$）。一般来说，这个值会经过 softmax 函数后，转化为被分类对象属于这个类别的 “概率” (sofrmax probability score)，记为 $p_i$，计算方法为：

    ![Formula 1](https://latex.codecogs.com/png.latex?P_i=\frac{e^{z_i}}{\sum_{j}{e^{z_j}}})

- 在温度缩放中，引入一个温度参数 $T$（简单来说就是**调整原始 logit score 的大小，起一个 scaling 的作用**），放入 softmax 函数重新计算输出：

    ![Formula 2](https://latex.codecogs.com/png.latex?P_i(T)=\frac{e^{z_i/T}}{\sum_{j}{e^{z_j/T}}})
​

&nbsp;

**为什么叫温度缩放？**

- 借鉴了物理学中的热力学概念：
    - **高温（T > 1）**：类似于在高温下，系统的能量分布更均匀。在温度标定中，较高的温度参数会让 softmax 输出更加平滑，降低最高类别的置信度，使得模型的概率分布更均匀。
    - **低温（T < 1）**：在低温下，能量集中在少数几个状态。类似地，低温参数会使得 softmax 输出变得更加尖锐，强化模型对某些类别的高置信度。
    - **T = 1**：这时候 logit score 和原始值一模一样，温度不起任何作用。

&nbsp;

### 2. 训练温度参数
温度参数 $T$ 通常通过在验证集上最小化负对数似然（Negative Log Likelihood, NLL）来进行优化。

![Formula 2](https://latex.codecogs.com/png.latex?NLL=-\sum_{i}{log(P_i)})

具体步骤如下：
1. 将数据集分为训练集和验证集。在训练集上训练模型，模型输出为 logits scores。
2. 在验证集上，通过最小化 NLL 来找到最优的温度参数（计算 NLL 的损失函数，最小化这个损失函数）。

&nbsp;

### 3. 优缺点分析
- 优点：
    - 无需重新训练模型，只需在模型输出的 logits 上进行后处理。
    - 对大多数分类问题，特别是深度学习模型（如神经网络）输出的置信度，温度标定通常能显著提高输出的校准性。
- 局限性：
    - 专门针对 softmax 输出，适用于分类问题。如果模型的输出并非 softmax 概率分布，则无法使用。
    - 假设所有样本共享同一个温度参数，可能不适用于复杂的多模态数据。
    - 主要目的是校准置信度，不能提高分类准确率。

&nbsp;

### 4. 如果标定后的温度参数等于1
- 情况1：如果模型在训练过程中已经很好地校准了输出概率，那么温度缩放可能不会进一步改善校准效果。
    - 解决方案：无。
- 情况2：验证集与训练集非常相似，导致模型在验证集上的表现与训练集一致，从而不需要额外的校准。
    - 解决方案：更新验证集。
- 情况3：在优化温度参数的过程中，可能由于初始值、优化算法或其他因素，导致优化过程没有找到更优的温度参数。
    - 解决方案：重新设定优化的起始值以及步长，必要时可以改变优化算法。
- 情况4：温度缩放不适用于你的情况。
    - 解决方案：尝试其他校准方法，如Platt Scaling、Isotonic Regression等。